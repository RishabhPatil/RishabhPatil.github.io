---
layout: post
title:  "Hidden Markov Models"
date:   2018-10-01 21:30:00 -0700
categories: HMM hidden markov model
---
Work in Progress
<!-- <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
I worked on this for my Fundamentals of Statistical Learning course.<br>
This post assumes that you know what HMMs are and how they function, this is just a commentary of the implementation.<br>
In case you want to deep dive into the math, <a href='https://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf'>this is a wonderful resource.</a><br>
<br>
The problems we are trying to solve:<br>
<b>Problem 1: Observation Likelihood</b>
<br>
Calculate the likelyhood of seeing a set of observations given the parameters of a HMM.

<b>Problem 3: Optimize model for Observations</b><br>
Find the model parameters that best describe the input observations.

<hr>
<h2>Descriptors of a HMM:</h2>
$$ M = \text{Number of unique observation symbols} $$<br>
$$ V = \{ v_1, v_2 ... v_M \} = \text{Set of observation symbols} $$<br><br>
$$ t = \text{time instant; The symbol observed at time t is } v_t$$<br><br>
$$ N = \text{Number of states in HMM} $$<br>
$$ S = \{ S_1, S_2 ... S_N \} = \text{Set of states} $$<br><br>
$$ \pi_i = \text{Initial State Distribution: Probability of starting in state i} $$ <br><br>
$$ A = \{ a_{ij} \} = \text{State Transition Matrix} $$ <br>
$$ a_{ij} = \text{Probability of going from state } S_i \text{ to } S_j $$ <br><br>
$$ B = \{ b_{jk} \} = \text{Emission Matrix} $$ <br>
$$ b_{jk} = \text{Probability of observing symbol } V_k \text{ when in } S_j $$<br><br>
$$ T = \text{Number of observations in an observations sequence} $$<br>
$$ O = O_1 O_2 O_3 ... O_T = \text{Observation Sequence} $$<br><br>
<b>NOTE: Since I am using zero-based numbering, all variables start from 0. For example, t $$\in$$ [0,T)</b><br><br>

<hr>
<h2>Implementation</h2>

<h4>Initialize variables:</h4>
The HMM has 2 null states, the initial state and the final state and it generates the '0' symbol when in these states.<br>
```python
string = "0ABCD0"
vocab = {'A':0,'B':1,'C':2,'D':3, '0':4}
states = 3 	#5 counting the null states
symbols = 5
```
<h4>Generating transition and emission matrix:</h4><br>
We can include $$pi$$ in the transition matrix as $$A_{0i}$$, and we set<br>
$$A_{i0} = 0 $$; Probability of entering start state<br>
$$A_{Fi} = 0 $$; Probability of transitioning away from final state<br>
$$A_{0F} = 0 $$; Probability of directly transitioning from initial state to final state i.e. empty input

```python
# Make sure we generate the same random values everytime
random.seed(7)

#Generate random transition and emission probabilities
A = [[0]+[random.random() for i in range(states)]+[0]]
B = [[0 for i in range(symbols)]]
for i in range(states):
    transition_probabilities = [0]+[random.random() for i in range(states+1)]
    emission_probabilities = [random.random() for i in range(symbols)]
    B.append(emission_probabilities)
    A.append(transition_probabilities)
A.append([0 for i in range(states+1)]+[1])
B.append([0 for i in range(symbols)])
B[0][-1] = 1
B[-1][-1] = 1
A = np.array(A)
B = np.array(B)
```
<br>
We need to make sure that $$\sum_{j} A_{ij} = 1$$ and $$\sum_{k} B_{jk} = 1$$

```python
#normalize A and B
for i in range(len(A)):
    s = sum(A[i])
    A[i] = A[i]/s
    s = sum(B[i])
    if s!=0:
        B[i] = B[i]/s
        
print("Transition Probabilities:")
print(A)
print("Emission Probabilities:")
print(B)
```
Which gives us this:
~~~~
Transition Probabilities:
[[0.         0.28769371 0.13401473 0.57829156 0.        ]
 [0.         0.07018978 0.51926242 0.35434762 0.05620017]
 [0.         0.26558805 0.51729592 0.07745309 0.13966295]
 [0.         0.03479135 0.64116701 0.21630139 0.10774024]
 [0.         0.         0.         0.         1.        ]]
Emission Probabilities:
[[0.         0.         0.         0.         1.        ]
 [0.44545295 0.0329156  0.38067629 0.06132265 0.07963251]
 [0.17798612 0.26883982 0.16370876 0.1125277  0.27693761]
 [0.05875725 0.15387723 0.407101   0.09015012 0.2901144 ]
 [0.         0.         0.         0.         1.        ]]
~~~
<br>
<h4> Forward Procedure:</h4>
We define $$\alpha_t(i)$$ as the probability of observing $$O_1$$ through $$O_t$$ and being in state $$S_i$$ at time t given the model parameters,<br>
$$\alpha_t(i) = P(O_1, O_2 ... O_t, q_t = S_i | \lambda)$$<br><br>
At time $$t=0$$ it can defined as,<br>
$$\alpha_0(i) = \pi_i * b_i(O_0) $$
<br><br>
After $$t=0$$,<br>
$$\alpha_{t+1}(i) = [\sum_{j=0}^N \alpha_t(i)*a_{ij}]*b_j(O_{t+1}) $$

```python
length = len(string)

first_char = vocab[string[0]]
alpha = np.zeros((length,states+2))

alpha[0,:] = [ A[0][i]*B[i][ first_char ] for i in range(0,states+2) ]
for t in range(1,length):
    for j in range(0, states+2):
        s = 0
        for i in range(0,states+2):
            s += alpha[t-1][i]*A[i][j]
        alpha[t,j] = s*B[j][vocab[string[t]]]

print(alpha)
```
Result:
```
[
 [0.00000000e+00 2.29097717e-02 3.71137173e-02 1.67770710e-01 0.00000000e+00]
 [0.00000000e+00 7.70720986e-03 2.46802675e-02 2.77813860e-03 0.00000000e+00]
 [0.00000000e+00 2.36742384e-04 4.98706385e-03 8.06856711e-04 0.00000000e+00]
 [0.00000000e+00 5.21219343e-04 5.27150234e-04 2.62448579e-04 0.00000000e+00]
 [0.00000000e+00 1.13888439e-05 8.00764726e-05 2.54484994e-05 0.00000000e+00]
 [0.00000000e+00 1.82773505e-06 1.76281225e-05 4.56707533e-06 1.45655987e-05]
]
```
<br>
<h4> Backward Procedure:</h4>
As opposed to the Forward Procedure algorithm,
$$\beta_t(i)$$ is the probability of observing $$O_t+1$$ through $$O_T$$, given the state $$S_i$$ at time t and the model parameters,<br>
$$\beta_t(i) = P(O_{t+1}, O_{t+2} ... O_T | q_t = S_i, \lambda)$$
<br><br>
At time t=T it can be defined as,<br>
$$\beta_T(i) = 1$$<br>

For all other values of t,<br>
$$ \beta_t(i) = \sum_{j=0}^N a_{ij}*b_j(O_{t+1})*\beta_{t+1}(j) $$

```python
length = len(string)

beta = np.zeros((length,states+2))

beta[length-1,:] = [ 1 for i in range(states+2)]

for t in range(length-2,-1,-1):
    for i in range(0, states+2):
        s = 0
        for j in range(0, states+2):
            s += A[i][j]*B[j][vocab[string[t+1]]]*beta[t+1,j]
        beta[t,i] = s
print(beta)
```
Result:<br>
```
[
 [2.27676200e-04 1.61150960e-04 2.47104920e-04 1.53337956e-04 0.00000000e+00]
 [8.30943347e-04 1.23697750e-03 1.02951452e-03 1.31245889e-03 0.00000000e+00]
 [1.13552429e-02 7.57612526e-03 6.41815848e-03 5.93311738e-03 0.00000000e+00]
 [2.86547380e-02 3.16146272e-02 2.64803026e-02 3.10585744e-02 0.00000000e+00]
 [2.27794199e-01 3.08394203e-01 3.26541339e-01 3.50826173e-01 1.00000000e+00]
 [1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00]
]
```
<br>
<h4> Expectation Maximization: </h4>
To solve Problem 3 we use EM algorithm.
For which we need to calculate $$\gamma$$ and $$\xi$$.<br><br>
$$\xi_t(i,j)$$ can be defined as the probability of being in $$S_i$$ at time $$t$$ and $$S_j$$ at time $$t+1$$ given the entire observation sequence $$O$$ and the model parameters $$\lambda$$.<br><br>
$$\xi_t(i,j) = \frac{\alpha_t(i) * a_{ij} * b_j(O_{t+1}) * \beta_{t+1}(j)}{P(O|\lambda)}$$<br><br>
$$\xi_t(i,j) = \frac{\alpha_t(i) * a_{ij} * b_j(O_{t+1}) * \beta_{t+1}(j)}{ \sum_{i=0}^N \sum_{j=0}^N \alpha_t(i) * a_{ij} * b_j(O_{t+1}) * \beta_{t+1}(j) }$$<br><br>
$$\gamma_t(i)$$ can be defined as the probability of being in $$S_i$$ at time $$t$$ given the observation sequence $$O$$ and the model paramters $$\lambda$$. Which can be defined in terms of $$\xi$$ as:<br><br>
$$\gamma_t(i) = \sum_{j=0}^N \xi_t(i,j)$$<br>

```python
def Xi(t,i,j,string):
    num = alpha[t,i] * A[i][j] * B[j][vocab[string[t+1]]] * beta[t+1][j]
    den = 0
    for ti in range(0,states+2):
        for tj in range(0,states+2):
            den += alpha[t,ti] * A[ti][tj] * B[tj][vocab[string[t+1]]] * beta[t+1][tj]
    return num/den

def gamma(t,i,string):
    g = 0
    for j in range(0,states+2):
        g += Xi(t,i,j,string)
    return g
```
<br>
Now we can obtain the new model parameters $$\hat\lambda = (\hat\pi, \hat A, \hat B)$$,<br><br>
$$\hat\pi_i = \text{frequency in } S_i \text{ at time } t=0$$<br>
$$\hat\pi_i = \gamma_0(i)$$<br><br>
$$\hat A_{ij} = \frac{\text{number of transitions from }S_i\text{ to }S_j}{\text{number of transitions from }S_i}$$<br>
$$\hat A_{ij} = \frac{ \sum_{t=0}^{T-1} \xi_t(i,j) } {\sum_{t=0}^{T-1} \gamma_t(i)}$$<br><br>
$$\hat B_j(k) = \frac{\text{number of times in }S_j\text{ and observing }V_k}{\text{number of times in }S_j}$$<br>
$$\hat B_j(k) = \frac{ \sum_{t=0\text{ s.t. O_t=V_k}}^{T} \gamma_t(j) } {\sum_{t=0}^{T} \gamma_t(j)}$$<br><br>

```python
def get_pi(string):
    pi = []
    for i in range(0,states+2):
        pi.append(gamma(0,i,string))
    return pi

def get_A(string):
    new_A = np.zeros((states+2, states+2))
    for i in range(0, states+2):
        for j in range(0, states+2):
            num = 0
            den = 0
            T = len(string)
            for t in range(0, T-1):
                num += Xi(t,i,j,string)
                den += gamma(t,i,string)
            if num:
                new_A[i][j] = num/den
            else:
                new_A[i][j] = 0
    new_A[-1,-1] = 1
    return new_A

def get_B(string):
    T = len(string)
    new_B = np.zeros((states+2,symbols))
    for j in range(0,states+2):
        for k in range(symbols):
            num = 0
            den = 0
            for t in range(0,T-1):
                if vocab[string[t]] == k:
                    num += gamma(t,j,string)
                den += gamma(t,j,string)
            if num:
                new_B[j][k] = num/den
            else:
                new_B[j][k] = 0
    new_B[0][vocab['0']] = 1
    new_B[-1][vocab['0']] = 1
    return new_B
```
Training for one string:
```python
nPi = get_pi("0ABCD0")
nA = get_A("0ABCD0")
nB = get_B("0ABCD0")
nA[0] = nPi
print("New Transition Matrix:\n",nA)
print("New Emission Matrix:\n",nB)
```

Result:
```
New Transition Matrix:
 [[0.         0.09567432 0.23766082 0.66666485 0.        ]
 [0.         0.05645478 0.60557462 0.31968832 0.01828228]
 [0.         0.25631546 0.54568847 0.09317639 0.10481968]
 [0.         0.07458077 0.69730925 0.17459865 0.05351133]
 [0.         0.         0.         0.         1.        ]]
New Emission Matrix:
 [[0.         0.         0.         0.         1.        ]
 [0.27231536 0.05123141 0.47067566 0.10032265 0.10545491]
 [0.23814365 0.29999363 0.13083201 0.24507543 0.08595527]
 [0.07116152 0.0934297  0.15908577 0.17424477 0.50207824]
 [0.         0.         0.         0.         1.        ]]
```
We can keep training the HMM multiple times, each time replacing $$\lambda$$ with $$\hat\lambda$$.

<br>
<h4> Observation Likelihood: </h4>
We can define the likelihood of seeing an observation sequence as,<br><br>
$$ P(O|\lambda) = \sum_{i=0}^N \alpha_T(i) $$<br><br>

```python
def prob(string):
	alpha = forward_procedure(string)
    length = len(string)
    s = 0
    for i in range(0,states+2):
        s += alpha[length-1][i]
    return s

print(prob('0AAA0')
``` 
Which returns:
```
0.00039031428207478964
```
<hr>
<br>

Maybe I should make a separate post for EM algorithm.<br>
I might keep adding comments, but that should be it for this post.<br>
Email me if you come across any issues. -->