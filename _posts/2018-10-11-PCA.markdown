---
layout: post
title:  "PCA, Whitening and ZCA"
date:   2018-10-01 21:30:00 -0700
categories: feature selection PCA whitening ZCA dimensionality reduction
---
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
[Complete Notebook with a small example of Fisher's Linear Discriminant included.](https://github.com/RishabhPatil/FundamentalsOfStatisticalLearning/blob/master/Assignment%202/PCA%20and%20Fisher's.ipynb)

PCA stands for Principal Component Analysis, or in simpler words: picking out the features which best represent the data.<br>
This is done by finding the "directions" (or features) along which the data has the most variance.<br>
Each new direction is perpendicular to the previously chosen directions or each extracted feature is independent of the rest.<br>
<br>
But why do we maximize variance?<br>
Intuitively we can say that if the value of a feature is "farther" from the other data points, it is easier to identify the point, makes it unique.<br>
Hence we try to maximize the distance from the mean (for each feature individually).<br>
<br>
PCA doesnt actually pick out the best features.<br>
For a set of points in a d-dimensional space, PCA will return $$ \text{d} $$ new dimensions which act as the new axes.<br>
These axes can be "sorted" by the variance in their direction.<br>
When we say we take the top k features, it means that we are taking components of the d - dimensional data onto the space defined by the k - axes (directions/dimensions with the highest variance).<br>
And since we are taking the "best" features, the amount of information lost is minimized.<br>

***WIP***